{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOMzZiVurHDKfmiNOfKX5I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bcarmoai/Colab/blob/main/Megaloop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJt1m15pTJRK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "os.chdir(r'U:\\Marketing and Risk\\Credit\\Auto-decisioning\\2017-18 Scorecard build\\Bernard\\segmentWOE')\n",
        "from segmentWOE import segmentWOE_ML\n",
        "os.chdir(r'U:\\Marketing and Risk\\Credit\\Auto-decisioning\\2017-18 Scorecard build\\Bernard\\dtreeAndROC')\n",
        "from dtreeAndROC import dtreeRegROCWOE_ML\n",
        "os.chdir(r'U:\\Marketing and Risk\\Credit\\Auto-decisioning\\2017-18 Scorecard build\\Bernard\\dtreeAndROC')\n",
        "\n",
        "# =============================================================================\n",
        "# Megaloop algorithm\n",
        "# 1) Create baseline model for non-segmented dataset\n",
        "# 2) For each variable in varlist that is not in ignore list\n",
        "# 2.1) Find a value that splits population in half or creates a min 30% subset\n",
        "# 2.2) Create dedicated models for each subset by Gini loop\n",
        "# 2.3) Concatenate model scores from both subsets to calculate Gini improvement\n",
        "# 2.4) Store Gini improvement and data required to plot ROC\n",
        "# =============================================================================\n",
        "\n",
        "def megaloop(train_to_split, valid_to_split, loop_var_list, train_var_list, segpath, segid, toignore_baseline=[], left_toignore=[],\n",
        "             right_toignore=[], minsample = 200, split_min_group = 0.30, debug_mode=False, min_Gini_diff=0.003, max_vif=2.9):\n",
        "\n",
        "# =============================================================================\n",
        "# Code for testing\n",
        "#     toignore_baseline=[]\n",
        "#     left_toignore=[]\n",
        "#     right_toignore=[]\n",
        "#     minsample = 200\n",
        "#     stratX_train_LimNonLim_ORI = pandas.read_csv(r\"U:\\Marketing and Risk\\Credit\\Auto-decisioning\\2017-18 Scorecard build\\4. Commercial model\\BC_ConComm_LimNonLim_Retro_Aug_train_stratX_train.csv\")\n",
        "#     stratX_test_LimNonLim_ORI = pandas.read_csv(r\"U:\\Marketing and Risk\\Credit\\Auto-decisioning\\2017-18 Scorecard build\\4. Commercial model\\BC_ConComm_LimNonLim_Retro_Aug_train_stratX_test.csv\")\n",
        "#     varlist_LimNonLim_ORI = ['RNILF04','CCJVAL24','GAZBKO36','EIOLF91','BSC423','LOCNCCJV','LSC251','SSC5','TSC135','INWORSTC','INTODFCU','IFOSF01']\n",
        "#     train_to_split = (stratX_train_LimNonLim_ORI.copy())[['OpportunityID','target']+varlist_LimNonLim_ORI]\n",
        "#     valid_to_split = stratX_test_LimNonLim_ORI.copy()[['OpportunityID','target']+varlist_LimNonLim_ORI]\n",
        "#     loop_var_list = varlist_LimNonLim_ORI\n",
        "#     loop_var_list = ['RNILF04']\n",
        "#     train_var_list = loop_var_list\n",
        "#     segpath=r\"U:\\Marketing and Risk\\Credit\\Auto-decisioning\\2017-18 Scorecard build\\4. Commercial model\\megaloop\";\n",
        "#     segid=\"TESTRUN\"\n",
        "#     var = 'RNILF04'\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "    var_gini_gain = []\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG: 001 Baseline')\n",
        "\n",
        "    # 1) Create baseline model for non-segmented dataset\n",
        "    # Replace raw values with WOE\n",
        "    train_to_split_woe, valid_to_split_woe = segmentWOE_ML(segid, train_to_split, valid_to_split, toignore_baseline, segpath, msample=minsample,\n",
        "                                                           varID='Baseline', debug_mode=debug_mode)\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG: 002 Baseline')\n",
        "\n",
        "    # =============================================================================\n",
        "    # Code for testing\n",
        "    #     train_to_split_woe=stratX_train_LimNonLim_ORI\n",
        "    #     valid_to_split_woe=stratX_test_LimNonLim_ORI\n",
        "    # =============================================================================\n",
        "\n",
        "    # Created baseline model on WOE vars\n",
        "    (baseline_scored_train, baseline_scored_valid, baseline_logreg_model, baseline_logreg_varlist) = ml_logreg(\n",
        "            train_to_split_woe, valid_to_split_woe, train_var_list, debug_mode=debug_mode, min_Gini_diff=min_Gini_diff, max_vif=max_vif)\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG: 003 Baseline')\n",
        "\n",
        "    ml_save_train_valid_seg(baseline_scored_train, baseline_scored_valid, None, segpath, segid, 'baseline')\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG: 001')\n",
        "\n",
        "    # 2) For each variable in varlist that is not in ignore list\n",
        "    for var in loop_var_list:\n",
        "\n",
        "        try:\n",
        "\n",
        "            if debug_mode:\n",
        "                print('DEBUG: 001a')\n",
        "\n",
        "            # 2.1) Find a value that splits population in half or creates a min x% subset\n",
        "            left_subset_train, right_subset_train, split_val = ml_split_train(var, train_to_split, min_group=split_min_group, debug_mode=debug_mode)\n",
        "            if debug_mode:\n",
        "                print('DEBUG: 001aa')\n",
        "            left_subset_valid, right_subset_valid = ml_split_valid(var, valid_to_split, split_val, debug_mode=debug_mode)\n",
        "            if (left_subset_train is None) | (right_subset_train is None) | (left_subset_valid is None) | (right_subset_valid is None):\n",
        "                print(\"NOTE: no minimum split found for var \"+var)\n",
        "                continue\n",
        "\n",
        "            if debug_mode:\n",
        "                print('DEBUG: 001b')\n",
        "\n",
        "            # 2.2) Create dedicated models for each subset by Gini loop\n",
        "            # Replace raw values with WOE\n",
        "            left_subset_train_woe,  left_subset_valid_woe  = segmentWOE_ML(segid, left_subset_train,  left_subset_valid,  left_toignore,  segpath, msample=minsample, debug_mode=debug_mode)\n",
        "            right_subset_train_woe, right_subset_valid_woe = segmentWOE_ML(segid, right_subset_train, right_subset_valid, right_toignore, segpath, msample=minsample, debug_mode=debug_mode)\n",
        "            # =============================================================================\n",
        "            # Code for testing\n",
        "            #             left_subset_train_woe = left_subset_train\n",
        "            #             left_subset_valid_woe = left_subset_valid\n",
        "            #             right_subset_train_woe= right_subset_train\n",
        "            #             right_subset_valid_woe= right_subset_valid\n",
        "            # =============================================================================\n",
        "\n",
        "            if debug_mode:\n",
        "                print('DEBUG: 002')\n",
        "\n",
        "            # Created models on WOE vars\n",
        "            left_scored_train,  left_scored_valid,  left_logreg_model,  left_logreg_varlist  = ml_logreg(left_subset_train_woe,  left_subset_valid_woe,  train_var_list, debug_mode=debug_mode, min_Gini_diff=min_Gini_diff, max_vif=max_vif)\n",
        "            right_scored_train, right_scored_valid, right_logreg_model, right_logreg_varlist = ml_logreg(right_subset_train_woe, right_subset_valid_woe, train_var_list, debug_mode=debug_mode, min_Gini_diff=min_Gini_diff, max_vif=max_vif)\n",
        "\n",
        "            if debug_mode:\n",
        "                print('DEBUG: 003')\n",
        "\n",
        "            # 2.3) Concatenate model scores from both subsets to calculate Gini improvement\n",
        "            gini_gain_train, gini_gain_valid, train_scored_LR, valid_scored_LR = ml_ginigain(\n",
        "                    baseline_scored_train, baseline_scored_valid, left_scored_train, left_scored_valid, right_scored_train, right_scored_valid,\n",
        "                    segpath, segid, var)\n",
        "\n",
        "            if debug_mode:\n",
        "                print('DEBUG: 004')\n",
        "\n",
        "            # 2.4) Store cumulative Gini improvement and data required to plot ROC\n",
        "            temp=[var, gini_gain_train, gini_gain_valid]\n",
        "            var_gini_gain.append(temp)\n",
        "            ml_save_train_valid_seg(train_scored_LR, valid_scored_LR, var_gini_gain, segpath, segid, var)\n",
        "\n",
        "            if debug_mode:\n",
        "                print('DEBUG: 005')\n",
        "\n",
        "        except:\n",
        "            if debug_mode:\n",
        "                print('DEBUG: 006')\n",
        "            temp=[var, np.nan, np.nan]\n",
        "            var_gini_gain.append(temp)\n",
        "            ml_save_train_valid_seg(None, None, var_gini_gain, segpath, segid, var)\n",
        "            continue\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG: 007')\n",
        "\n",
        "    df_var_gini_gain = pd.DataFrame(var_gini_gain, columns = [\"varname\",\"Train Gini Gain\",\"Valid Gini Gain\"]).sort_values(by='Valid Gini Gain', ascending='False')\n",
        "\n",
        "    return df_var_gini_gain\n",
        "\n",
        "\n",
        "def ml_save_train_valid_seg(train_scored_LR, valid_scored_LR, var_gini_gain, segpath, segid, var):\n",
        "\n",
        "    if ((train_scored_LR is None) | (valid_scored_LR is None)) == False:\n",
        "        train_scored_LR['source']='Train'\n",
        "        valid_scored_LR['source']='Valid'\n",
        "        TV_scored_LR = pandas.concat([train_scored_LR, valid_scored_LR])\n",
        "        os.chdir(segpath)\n",
        "        TV_scored_LR.to_csv('TV_Scored_LR_'+segid+'_'+var+'.csv', index=False)\n",
        "    else:\n",
        "        print(\"WARNING: ml_save_train_valid_seg received empty tables\")\n",
        "\n",
        "    if (var_gini_gain is None) == False:\n",
        "        pandas.DataFrame(data=var_gini_gain, columns = [\"varname\",\"Train Gini Gain\",\"Valid Gini Gain\"]).to_csv('VarGiniGain_'+segid+'_accum_loop.csv', index=False)\n",
        "\n",
        "\n",
        "def ml_split_train(var, train_to_split, min_group = 0.30, debug_mode=False):\n",
        "\n",
        "# =============================================================================\n",
        "# Code for testing\n",
        "# dfCopy = stratX_train_LimNonLim_ORI.copy()\n",
        "# A,C = ml_split_train('RNILF04', dfCopy)\n",
        "# B=A['SplitSide'].value_counts(dropna=False)\n",
        "# A==[]\n",
        "# dfCopy2 = stratX_train_LimNonLim_ORI.copy()\n",
        "# dfCopy2['RNILF04'].value_counts(dropna=False)\n",
        "# dfCopy2['RNILF04'] = np.where( dfCopy2['RNILF04']<-1.0, np.nan, dfCopy2['RNILF04'])\n",
        "# dfCopy2['RNILF04'].value_counts(dropna=False)\n",
        "# A,C = ml_split_train('RNILF04', dfCopy2)\n",
        "# B=A['SplitSide'].value_counts(dropna=False).reset_index()\n",
        "# train_to_split = dfCopy2\n",
        "# var = 'RNILF04'\n",
        "# group_sizes['left'] = (group_sizes['index'])[0].closed\n",
        "# A=group_sizes.sort_values(by='index', ascending='True')\n",
        "# =============================================================================\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG ml_split_train: 001')\n",
        "\n",
        "    split_copy = train_to_split.copy()\n",
        "    split_copy['SplitSide']=pandas.qcut(train_to_split[var], 2, duplicates='drop')\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG ml_split_train: 002')\n",
        "\n",
        "    overall_size = split_copy.shape[0]\n",
        "    group_sizes = split_copy['SplitSide'].value_counts(dropna=False).reset_index()\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG ml_split_train: 003')\n",
        "\n",
        "    # Allocate NaN to smallest group\n",
        "    if (group_sizes.loc[group_sizes['index'].isnull()]).shape[0] > 0:\n",
        "        print(\"NOTE: ml_split_train \"+var+\" includes NaN\")\n",
        "        group_to_allocate_nan = ((group_sizes.loc[~group_sizes['index'].isnull()]).sort_values(by='SplitSide', ascending='False').iloc[0])['index']\n",
        "        #group_sizes['index'] = np.where(group_sizes['index'].isnull(),group_to_allocate_nan,group_sizes['index'])\n",
        "        split_copy['SplitSide'] = np.where(split_copy['SplitSide'].isnull(),group_to_allocate_nan,split_copy['SplitSide'])\n",
        "        group_sizes = split_copy['SplitSide'].value_counts(dropna=False).reset_index()\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG ml_split_train: 004')\n",
        "\n",
        "    group_sizes['ColPct'] = group_sizes['SplitSide']/overall_size\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG ml_split_train: 005')\n",
        "\n",
        "    if min(group_sizes['ColPct']) < min_group:\n",
        "        print('ERROR ml_split_train: group_sizes[ColPct]) < min_group for '+var)\n",
        "        return None, None\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG ml_split_train: 006')\n",
        "\n",
        "    if group_sizes.shape[0] > 1:\n",
        "        try:\n",
        "            #group_sizes=group_sizes.loc[~group_sizes['index'].isnull()]\n",
        "            group_sizes['left_lim']=0.0\n",
        "            (group_sizes['left_lim'])[0]=(group_sizes['index'])[0].left\n",
        "            (group_sizes['left_lim'])[1]=(group_sizes['index'])[1].left\n",
        "            #(group_sizes['left_lim'])[2]=(group_sizes['index'])[2].left\n",
        "            group_sizes.sort_values(by='left_lim', inplace=True)\n",
        "            group_sizes.reset_index(drop=True, inplace=True)\n",
        "            left_subset_train  = split_copy.loc[split_copy['SplitSide']==(group_sizes['index'])[0]]\n",
        "            right_subset_train = split_copy.loc[split_copy['SplitSide']==(group_sizes['index'])[1]]\n",
        "            left_subset_train.drop(['SplitSide'], axis=1, inplace=True)\n",
        "            right_subset_train.drop(['SplitSide'], axis=1, inplace=True)\n",
        "            split_val_lowest = (group_sizes['index'])[0].left\n",
        "            split_val = (group_sizes['index'])[0].right\n",
        "            split_val_highest = (group_sizes['index'])[1].right\n",
        "\n",
        "            if ~((split_val_lowest<=split_val) & (split_val<=split_val_highest)):\n",
        "                print('ERROR: ~((split_val_lowest<=split_val) & (split_val<=split_val_highest))')\n",
        "                print('split_val_lowest=',split_val_lowest,' split_val=',split_val,' split_val_highest=',split_val_highest)\n",
        "                return None, None, np.NaN\n",
        "\n",
        "            if (((group_sizes['index'])[0].closed == 'right') == False):\n",
        "                print('ERROR: (((group_sizes[\\'index\\'])[0].closed == \\'right\\') == False)')\n",
        "                return None, None, np.NaN\n",
        "\n",
        "        except:\n",
        "            print('ERROR: ml_split_train  group_sizes.shape[0] > 1 generated an exception')\n",
        "            return None, None, np.NaN\n",
        "    else:\n",
        "        try:\n",
        "            left_subset_train  = split_copy.loc[split_copy[var]==1]\n",
        "            right_subset_train = split_copy.loc[split_copy[var]==2]\n",
        "            left_subset_train.drop(['SplitSide'], axis=1, inplace=True)\n",
        "            right_subset_train.drop(['SplitSide'], axis=1, inplace=True)\n",
        "            split_val=1\n",
        "        except:\n",
        "            print('ERROR: ml_split_train has no valid values to split')\n",
        "            return None, None, np.NaN\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG ml_split_train: 007')\n",
        "\n",
        "    return left_subset_train, right_subset_train, split_val\n",
        "\n",
        "\n",
        "def ml_split_valid(var, valid_to_split, split_val, debug_mode=False):\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG ml_split_valid: 001')\n",
        "\n",
        "    if ((valid_to_split is None) | (split_val == np.NaN) )==True:\n",
        "        print(\"WARNING: ml_split_valid called with blanks\")\n",
        "        return None, None\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG ml_split_valid: 002')\n",
        "\n",
        "    left_subset_valid  = valid_to_split.loc[valid_to_split[var] <= split_val]\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG ml_split_valid: 003')\n",
        "\n",
        "    right_subset_valid = valid_to_split.loc[valid_to_split[var] >  split_val]\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG ml_split_valid: 004')\n",
        "\n",
        "    return left_subset_valid, right_subset_valid\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "def ml_ginigain(baseline_scored_train, baseline_scored_valid, left_scored_train, left_scored_valid, right_scored_train, right_scored_valid, segpath, segid, var):\n",
        "\n",
        "# =============================================================================\n",
        "# Code for testing\n",
        "#     left_scored_train  = pandas.merge(stratX_train_S001[['OpportunityID','target']], scored_train_S001, how = 'inner', left_index=True, right_index=True).rename(columns={0:'PD'})\n",
        "#     right_scored_train = pandas.merge(stratX_train_S002[['OpportunityID','target']], scored_train_S002, how = 'inner', left_index=True, right_index=True).rename(columns={0:'PD'})\n",
        "#     left_scored_valid  = pandas.merge(stratX_test_S001[['OpportunityID','target']],  scored_test_S001,  how = 'inner', left_index=True, right_index=True).rename(columns={0:'PD'})\n",
        "#     right_scored_valid = pandas.merge(stratX_test_S002[['OpportunityID','target']],  scored_test_S002,  how = 'inner', left_index=True, right_index=True).rename(columns={0:'PD'})\n",
        "#     baseline_scored_train = pandas.merge(stratX_train_LimNonLim_ORI[['OpportunityID','target']],  logRegTrainScored_LimNonLim_ORI,  how = 'inner', left_index=True, right_index=True).rename(columns={0:'PD'})\n",
        "#     baseline_scored_valid = pandas.merge(stratX_test_LimNonLim_ORI[['OpportunityID','target']],   logRegTestScored_LimNonLim_ORI,   how = 'inner', left_index=True, right_index=True).rename(columns={0:'PD'})\n",
        "#     segpath=r\"U:\\Marketing and Risk\\Credit\\Auto-decisioning\\2017-18 Scorecard build\\4. Commercial model\";\n",
        "#     segid=\"TESTRUN\"\n",
        "#     var=\"Thickness\"\n",
        "# =============================================================================\n",
        "\n",
        "    train_scored_LR = pandas.concat([left_scored_train, right_scored_train])\n",
        "    valid_scored_LR = pandas.concat([left_scored_valid, right_scored_valid])\n",
        "\n",
        "     # Plot all models together\n",
        "    mult=-1.0\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    lw = 2\n",
        "\n",
        "    # Reg Train ORI\n",
        "    fpr1, tpr1, thresholds1 = metrics.roc_curve(baseline_scored_train['target'], baseline_scored_train['PD']*mult, pos_label=0)\n",
        "    roc_auc1 = auc(fpr1, tpr1)\n",
        "    Gini_train_ORI = 2*roc_auc1-1\n",
        "    Nconsumer = baseline_scored_train.shape[0]\n",
        "    plt.plot(fpr1, tpr1, ls='dashed', color='xkcd:light purple',\n",
        "             lw=1, label='Non-segmented Train (AUC = %0.2f Gini = %0.2f N=%0.0f)' %(roc_auc1,Gini_train_ORI,Nconsumer))\n",
        "\n",
        "    # Reg Valid ORI\n",
        "    fpr1, tpr1, thresholds1 = metrics.roc_curve(baseline_scored_valid['target'], baseline_scored_valid['PD']*mult, pos_label=0)\n",
        "    roc_auc1 = auc(fpr1, tpr1)\n",
        "    Gini_valid_ORI = 2*roc_auc1-1\n",
        "    Nconsumer = baseline_scored_valid.shape[0]\n",
        "    plt.plot(fpr1, tpr1, ls='solid', color='xkcd:light purple',\n",
        "             lw=2, label='Non-segmented Valid (AUC = %0.2f Gini = %0.2f N=%0.0f)' %(roc_auc1,Gini_valid_ORI,Nconsumer))\n",
        "\n",
        "    # Reg Train SEG\n",
        "    fpr1, tpr1, thresholds1 = metrics.roc_curve(train_scored_LR['target'], train_scored_LR['PD']*mult, pos_label=0)\n",
        "    roc_auc1 = auc(fpr1, tpr1)\n",
        "    Gini_train_LR = 2*roc_auc1-1\n",
        "    Nconsumer = train_scored_LR.shape[0]\n",
        "    plt.plot(fpr1, tpr1, ls='dashed', color='xkcd:blue',\n",
        "             lw=1, label='Segmented Train (AUC = %0.2f Gini = %0.2f N=%0.0f)' %(roc_auc1,Gini_train_LR,Nconsumer))\n",
        "\n",
        "    # Reg Valid SEG\n",
        "    fpr1, tpr1, thresholds1 = metrics.roc_curve(valid_scored_LR['target'], valid_scored_LR['PD']*mult, pos_label=0)\n",
        "    roc_auc1 = auc(fpr1, tpr1)\n",
        "    Gini_valid_LR = 2*roc_auc1-1\n",
        "    Nconsumer = valid_scored_LR.shape[0]\n",
        "    plt.plot(fpr1, tpr1, ls='solid', color='xkcd:blue',\n",
        "             lw=2, label='Segmented Valid (AUC = %0.2f Gini = %0.2f N=%0.0f)' %(roc_auc1,Gini_valid_LR,Nconsumer))\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Megaloop ROC SegID:'+segid+' Var:'+var)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    os.chdir(segpath)\n",
        "    plt.savefig('ROC_'+segid+'_'+var+'.png')\n",
        "\n",
        "    gini_gain_train = Gini_train_LR - Gini_train_ORI\n",
        "    gini_gain_valid = Gini_valid_LR - Gini_valid_ORI\n",
        "\n",
        "    return gini_gain_train, gini_gain_valid, train_scored_LR, valid_scored_LR\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from segmentWOE import fVarnameGini_ML, fVarnameGini_TT\n",
        "os.chdir(r'U:\\Marketing and Risk\\Credit\\Auto-decisioning\\2017-18 Scorecard build\\Bernard\\segmentWOE')\n",
        "from segmentWOE_EG_v2 import fVarnameGini_CV\n",
        "\n",
        "def ml_logreg(train_df, valid_df, train_var_list, max_var_count=15, max_vif=2.9, min_Gini_diff = 0.003, debug_mode=False, test_predict=True, crossval_predict=False):\n",
        "\n",
        "# =============================================================================\n",
        "# Code for testing\n",
        "#     max_var_count=15\n",
        "#     max_vif=2.9\n",
        "#     min_Gini_diff = 0.003\n",
        "#     debug_mode=False\n",
        "#     train_df=left_subset_train_woe\n",
        "#     valid_df=left_subset_valid_woe\n",
        "#     train_var_list=varlist_reduced\n",
        "#     train_var_list=varlist_LimNonLim_ORI\n",
        "# =============================================================================\n",
        "\n",
        "    varlist = []\n",
        "    targetvar=['target']\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG: 001')\n",
        "\n",
        "    # Create initial model on top varnamesGini contributor\n",
        "    if crossval_predict==False:\n",
        "        varnamesGini = fVarnameGini_TT(train_df, train_df['target'], valid_df, valid_df['target'], varlist, train_var_list, test_predict)\n",
        "    else:\n",
        "        varnamesGini = fVarnameGini_CV(train_df, train_df['target'], varlist, train_var_list, 5)\n",
        "        varnamesGini['Gini']=varnamesGini['cv_gini']\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG: 001a')\n",
        "\n",
        "    varlist = [varnamesGini['var'].iloc[0]]\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG: 001b')\n",
        "\n",
        "    logreg_model, logreg_gini_train, logreg_gini_valid, train_proba, valid_proba = dtreeRegROCWOE_ML(train_df[varlist+targetvar], valid_df[varlist+targetvar])\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG: 002')\n",
        "\n",
        "    # Add more vars while Gini improvement is significant or we have max variables\n",
        "    while True:\n",
        "        # Assess Gini improvement of next var and stop if low improvement\n",
        "        if crossval_predict==False:\n",
        "            varnamesGini = fVarnameGini_TT(train_df, train_df['target'], valid_df, valid_df['target'], varlist, train_var_list, test_predict)\n",
        "        else:\n",
        "            varnamesGini = fVarnameGini_CV(train_df, train_df['target'], varlist, train_var_list, 5)\n",
        "            varnamesGini['Gini']=varnamesGini['cv_gini']\n",
        "        if ((varnamesGini['Gini'].iloc[0] - logreg_gini_valid) < min_Gini_diff):\n",
        "            break\n",
        "        candidate_var = [varnamesGini['var'].iloc[0]]\n",
        "        candidate_varlist = varlist + candidate_var\n",
        "        vif = [variance_inflation_factor(train_df[candidate_varlist].values, i) for i in range(train_df[candidate_varlist].shape[1])]\n",
        "        # Do not add new var if vif threshold is breached\n",
        "        if max(pd.DataFrame(vif)[0]) > max_vif:\n",
        "            break\n",
        "        # Create model including added new var\n",
        "        varlist = candidate_varlist\n",
        "        (logreg_model, logreg_gini_train, logreg_gini_valid,\n",
        "         train_proba, valid_proba) = dtreeRegROCWOE_ML(\n",
        "                 train_df[varlist+targetvar], valid_df[varlist+targetvar])\n",
        "        if len(varlist)>=max_var_count:\n",
        "            break\n",
        "\n",
        "    if debug_mode:\n",
        "        print('DEBUG: 003')\n",
        "\n",
        "    # Merge PDs with target values so concatenated Ginis can be computed outside of this function\n",
        "    scored_train = pandas.merge((train_df.reset_index())[['OpportunityID','target']], train_proba, how = 'inner', left_index=True, right_index=True).rename(columns={0:'PD'})\n",
        "\n",
        "# =============================================================================\n",
        "#     # -- Check that the merge worked properly\n",
        "#     from sklearn import metrics\n",
        "#     import matplotlib.pyplot as plt\n",
        "#     plt.figure()\n",
        "#     lw = 2\n",
        "#     fpr1, tpr1, thresholds1 = metrics.roc_curve(scored_train['target'], scored_train['PD']*-1.0, pos_label=0)\n",
        "#     roc_auc1 = auc(fpr1, tpr1)\n",
        "#     Gini2 = 2*roc_auc1-1\n",
        "#     Nconsumer = scored_train.shape[0]\n",
        "#     plt.plot(fpr1, tpr1, color='blue',\n",
        "#              lw=1, label='Reg on merged (AUC = %0.2f Gini = %0.2f N=%0.0f)' %(roc_auc1,Gini2,Nconsumer))\n",
        "#     fpr1, tpr1, thresholds1 = metrics.roc_curve(scored_train['target'],\n",
        "#                                                 logreg_model.predict_proba(train_df[varlist])[:,1]*-1.0, pos_label=0)\n",
        "#     roc_auc1 = auc(fpr1, tpr1)\n",
        "#     Gini2 = 2*roc_auc1-1\n",
        "#     Nconsumer = scored_train.shape[0]\n",
        "#     plt.plot(fpr1, tpr1, color='green',\n",
        "#              lw=1, label='Reg on fitter (AUC = %0.2f Gini = %0.2f N=%0.0f)' %(roc_auc1,Gini2,Nconsumer))\n",
        "#     plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "#     plt.xlim([0.0, 1.0])\n",
        "#     plt.ylim([0.0, 1.05])\n",
        "#     plt.xlabel('False Positive Rate')\n",
        "#     plt.ylabel('True Positive Rate')\n",
        "#     plt.title('Model ROC')\n",
        "#     plt.legend(loc=\"lower right\")\n",
        "#     plt.show()\n",
        "# =============================================================================\n",
        "\n",
        "    scored_valid = pandas.merge((valid_df.reset_index())[['OpportunityID','target']], valid_proba, how = 'inner', left_index=True, right_index=True).rename(columns={0:'PD'})\n",
        "\n",
        "# =============================================================================\n",
        "#     # -- Check that the merge worked properly\n",
        "#     from sklearn import metrics\n",
        "#     import matplotlib.pyplot as plt\n",
        "#     plt.figure()\n",
        "#     lw = 2\n",
        "#     fpr1, tpr1, thresholds1 = metrics.roc_curve(scored_valid['target'], scored_valid['PD']*-1.0, pos_label=0)\n",
        "#     roc_auc1 = auc(fpr1, tpr1)\n",
        "#     Gini2 = 2*roc_auc1-1\n",
        "#     Nconsumer = scored_valid.shape[0]\n",
        "#     plt.plot(fpr1, tpr1, color='blue',\n",
        "#              lw=1, label='Reg on merged (AUC = %0.2f Gini = %0.2f N=%0.0f)' %(roc_auc1,Gini2,Nconsumer))\n",
        "#     fpr1, tpr1, thresholds1 = metrics.roc_curve(scored_valid['target'],\n",
        "#                                                 logreg_model.predict_proba(valid_df[varlist])[:,1]*-1.0, pos_label=0)\n",
        "#     roc_auc1 = auc(fpr1, tpr1)\n",
        "#     Gini2 = 2*roc_auc1-1\n",
        "#     Nconsumer = scored_valid.shape[0]\n",
        "#     plt.plot(fpr1, tpr1, color='green',\n",
        "#              lw=1, label='Reg on fitter (AUC = %0.2f Gini = %0.2f N=%0.0f)' %(roc_auc1,Gini2,Nconsumer))\n",
        "#     plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "#     plt.xlim([0.0, 1.0])\n",
        "#     plt.ylim([0.0, 1.05])\n",
        "#     plt.xlabel('False Positive Rate')\n",
        "#     plt.ylabel('True Positive Rate')\n",
        "#     plt.title('Model ROC')\n",
        "#     plt.legend(loc=\"lower right\")\n",
        "#     plt.show()\n",
        "# =============================================================================\n",
        "\n",
        "    logreg_varlist = varlist\n",
        "\n",
        "    return scored_train, scored_valid, logreg_model, logreg_varlist"
      ]
    }
  ]
}